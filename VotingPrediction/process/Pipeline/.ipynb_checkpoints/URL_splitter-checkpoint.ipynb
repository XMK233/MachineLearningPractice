{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## This pipeline is for some toy code snippets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many kernels in total in KernelInfo files? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "directory = \"J:\\\\EECS_6414\\\\Data\\\\19-02-03\"\n",
    "total_sum = 0\n",
    "for i in os.listdir(directory):\n",
    "    if \"KernelInfo_\" not in i:\n",
    "        continue\n",
    "#     with open(os.path.join(directory, i), \"r\", encoding= \"utf-8\") as f:\n",
    "#         l = json.load(f)\n",
    "#     total_sum += len(l)\n",
    "    print(i)\n",
    "# print(total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put dataset, competition and kernel info into database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tqdm\n",
    "directory = \"J:\\\\EECS_6414\\\\Data\\\\19-02-19\"\n",
    "database = \"pipeline1\"\n",
    "total_sum = 0\n",
    "# limit = 200\n",
    "flag = True\n",
    "import pymysql.cursors\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db=database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "last_kernelinfo_file = None\n",
    "last_sql = None\n",
    "\n",
    "try: \n",
    "    with connection.cursor() as cursor:\n",
    "        for i in tqdm.tqdm(os.listdir(directory)):\n",
    "            if \"KernelInfo_\" not in i:\n",
    "                continue\n",
    "#             if flag and i != \"KernelInfo_102.json\":\n",
    "#                 continue\n",
    "            flag = False\n",
    "            print(i)\n",
    "            last_kernelinfo_file = i\n",
    "            with open(os.path.join(directory, i), \"r\", encoding= \"utf-8\") as f:\n",
    "                l = json.load(f)\n",
    "            for kernel in l:\n",
    "#                 total_sum += 1\n",
    "#                 if total_sum >= limit:\n",
    "#                     break\n",
    "\n",
    "                info = kernel[\"info\"]\n",
    "\n",
    "                a_id = info[\"author\"][\"id\"]\n",
    "                a_name = info[\"author\"][\"userName\"]\n",
    "                last_sql = \"INSERT IGNORE INTO {}.author (id, name) VALUES (\\\"{}\\\", \\\"{}\\\");\".format(database, a_id, a_name)\n",
    "                cursor.execute(last_sql)\n",
    "                connection.commit()\n",
    "                # print(\"author: \", a_id, a_name)\n",
    "\n",
    "                k_id = info[\"id\"]\n",
    "                k_name = info[\"scriptUrl\"]\n",
    "                last_sql = \"INSERT IGNORE INTO {}.kernel (id, name) VALUES (\\\"{}\\\", \\\"{}\\\");\".format(database, k_id, k_name)\n",
    "                cursor.execute(last_sql)\n",
    "                connection.commit()\n",
    "                # print(\"kernels: \", k_id, k_name)\n",
    "\n",
    "                datasources = info[\"dataSources\"]\n",
    "                for datasource in datasources:\n",
    "                    ds_id = datasource[\"sourceId\"]\n",
    "                    t_name = datasource[\"dataSourceUrl\"]\n",
    "                    ds_name = t_name if not (t_name == None or t_name == \"\") else \">>|None|<<\"\n",
    "                    ds_type = datasource[\"sourceType\"]\n",
    "                    last_sql = \"INSERT IGNORE INTO {}.datasource (id, name, type) VALUES (\\\"{}\\\", \\\"{}\\\", \\\"{}\\\");\".format(database, ds_id, ds_name, ds_type)\n",
    "                    cursor.execute(last_sql)\n",
    "                    connection.commit()\n",
    "                    # print(\"datasourse: \", ds_id, ds_name, ds_type)\n",
    "\n",
    "                # print(\"\\n\\n\")\n",
    "\n",
    "#             if total_sum >= limit:\n",
    "#                 break\n",
    "finally:\n",
    "    print(\"{} is failed. last sql: {}\".format(last_kernelinfo_file, last_sql))\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the urls into separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "file_dir = \"C:\\\\Users\\\\xmk233\\\\PycharmProjects\\\\EECS6414\\\\process\\\\Pipeline\"\n",
    "file_name = \"datasource_datasets_noNone.csv\"\n",
    "split_dir = \"C:\\\\Users\\\\xmk233\\\\PycharmProjects\\\\EECS6414\\\\process\\\\Pipeline\\\\dataset_split\"\n",
    "split_file_name = \"dataset_split_{}.txt\"\n",
    "single_file_limit = 200\n",
    "if not os.path.exists(split_dir):\n",
    "    os.makedirs(split_dir)\n",
    "\n",
    "# counter = 0\n",
    "with open(join(file_dir, file_name), \"r\") as reader:\n",
    "    r = csv.reader(reader)\n",
    "    line_count = 0\n",
    "    file_count = 0\n",
    "    writer = open(join(split_dir, split_file_name.format(file_count)), \"w\")\n",
    "    for i in r:\n",
    "        if r.line_num == 1:\n",
    "            continue\n",
    "        url = i[1]\n",
    "        writer.write(url + \"\\n\")\n",
    "        line_count += 1\n",
    "        if line_count >= single_file_limit:\n",
    "            line_count = 0\n",
    "            writer.close()\n",
    "            file_count += 1\n",
    "            writer = open(join(split_dir, split_file_name.format(file_count)), \"w\")\n",
    "    writer.close()\n",
    "\n",
    "        # counter += 1\n",
    "        # if counter >= 10:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the kernel code urls: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 442/442 [00:27<00:00, 16.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, json, tqdm\n",
    "directory = \"J:\\\\EECS_6414\\\\Data\\\\19-02-19\"\n",
    "# total_sum = 0\n",
    "# limit = 2\n",
    "flag = True\n",
    "\n",
    "write_path = r\"C:\\Users\\xmk233\\PycharmProjects\\EECS6414\\process\\Pipeline\\code_split\"\n",
    "\n",
    "file_name = \"code_split_{}.csv\"\n",
    "file_count = 0\n",
    "write_file = open(os.path.join(write_path, file_name.format(file_count)), \"w\", encoding= \"utf-8\")\n",
    "line_count = 0\n",
    "line_limit = 200\n",
    "\n",
    "file_name_voted = \"code_split_voted_{}.csv\"\n",
    "file_count_voted = 0\n",
    "write_file_voted = open(os.path.join(write_path, file_name_voted.format(file_count_voted)), \"w\", encoding= \"utf-8\")\n",
    "line_count_voted = 0\n",
    "line_limit_voted = 200\n",
    "\n",
    "fail_log = \"code_url_extract_failed.txt\"\n",
    "fail_file = open(fail_log, \"w\")\n",
    "\n",
    "for i in tqdm.tqdm(os.listdir(directory)):\n",
    "    # total_sum += 1\n",
    "    # if total_sum >= limit:\n",
    "    #     break\n",
    "\n",
    "    if \"KernelInfo_\" not in i:\n",
    "        continue\n",
    "#             if flag and i != \"KernelInfo_102.json\":\n",
    "#                 continue\n",
    "    flag = False\n",
    "\n",
    "    last_kernelinfo_file = i\n",
    "    with open(os.path.join(directory, i), \"r\", encoding= \"utf-8\") as f:\n",
    "        kernels = json.load(f)\n",
    "    for kernel in kernels:\n",
    "        try:\n",
    "            info = kernel[\"info\"]\n",
    "\n",
    "            # a_id = info[\"author\"][\"id\"]\n",
    "            # a_name = info[\"author\"][\"userName\"]\n",
    "            # last_sql = \"INSERT IGNORE INTO {}.author (id, name) VALUES (\\\"{}\\\", \\\"{}\\\");\".format(database, a_id, a_name)\n",
    "            # cursor.execute(last_sql)\n",
    "            # connection.commit()\n",
    "            # # print(\"author: \", a_id, a_name)\n",
    "\n",
    "            code_id = info[\"scriptVersionId\"]\n",
    "            k_name = info[\"scriptUrl\"]\n",
    "            vote_number = info[\"totalVotes\"]\n",
    "            \n",
    "            if vote_number > 0:\n",
    "                write_file_voted.write(\"{},{}\\n\".format(k_name, code_id))\n",
    "                line_count_voted += 1\n",
    "                if line_count_voted >= line_limit_voted:\n",
    "                    write_file_voted.close()\n",
    "                    line_count_voted = 0\n",
    "                    file_count_voted += 1\n",
    "                    write_file_voted = open(os.path.join(write_path, file_name_voted.format(file_count_voted)), \"w\", encoding= \"utf-8\")                \n",
    "            else:\n",
    "                write_file.write(\"{},{}\\n\".format(k_name, code_id))\n",
    "                line_count += 1\n",
    "                if line_count >= line_limit:\n",
    "                    write_file.close()\n",
    "                    line_count = 0\n",
    "                    file_count += 1\n",
    "                    write_file = open(os.path.join(write_path, file_name.format(file_count)), \"w\", encoding= \"utf-8\")\n",
    "        except:\n",
    "            fail_file.write(\"{},{}\\n\".format(i, k_name))\n",
    "            continue\n",
    "\n",
    "write_file.close()\n",
    "write_file_voted.close()\n",
    "fail_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download code files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, wget, tqdm\n",
    "from os.path import join \n",
    "import shutil\n",
    "\n",
    "split_list_dir = r\"C:\\Users\\xmk233\\PycharmProjects\\EECS6414\\process\\Pipeline\\code_split\"\n",
    "files = os.listdir(split_list_dir)\n",
    "code_output_dir = r\"J:\\EECS_6414\\Data\\19-02-24\\code_split\"\n",
    "catalog_file_name = \"code_split_{}.csv\"\n",
    "#download_temp_path = r\"J:\\EECS_6414\\Data\\19-02-24\\code_split\\tmp\"\n",
    "\n",
    "for num in tqdm.tqdm(range(0, 1 + 1)):\n",
    "    file_name = catalog_file_name.format(num)\n",
    "    if not os.path.exists(join(split_list_dir, file_name)):\n",
    "        continue\n",
    "    code_path = join(code_output_dir, \"{}\".format(num))\n",
    "    if not os.path.exists(code_path):\n",
    "        os.makedirs(code_path)\n",
    "    file_path = join(split_list_dir, file_name)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        code_ids = csv.reader(f)\n",
    "        print(\"in file {}: \".format(num))\n",
    "        for code_id in tqdm.tqdm(code_ids):\n",
    "            name = code_id[0]\n",
    "            author = name.split(\"/\")[1]\n",
    "            id = code_id[1]\n",
    "            url = \"https://www.kaggle.com/kernels/scriptcontent/{}/download\".format(id)\n",
    "            code_name = wget.download(url)\n",
    "            new_file_name = \"{}_{}\".format(author, code_name)\n",
    "            shutil.move(code_name, os.path.join(code_path, new_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='test_pipeline',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "try:\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        # Read a single record\n",
    "#         sql = \"SELECT * FROM test_pipeline.author;\"\n",
    "#         cursor.execute(sql)\n",
    "#         result = cursor.fetchone()\n",
    "#         print(result)\n",
    "        cursor.execute(\"INSERT INTO test_pipeline.author (id, name) VALUES ('{}', '{}');\".format())\n",
    "        connection.commit()\n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if there is any urls in .txt files downloaded failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import os\n",
    "catalog_dir = \"/home/seasun/eecs6414/catalog/dataset_split/\"\n",
    "catalog_file_name = \"dataset_split_{}.txt\"\n",
    "webpage_dir = \"/home/seasun/eecs6414/webpage/dataset_split/\"\n",
    "failed_file_name = \"data_failded_{}.txt\"\n",
    "failed_pages = []\n",
    "\n",
    "# line_count = 0\n",
    "# writer = open(join(catalog_dir, failed_file_name).format(file_count),\n",
    "#               \"w\")\n",
    "writer = None\n",
    "\n",
    "for num in range(0, 78 + 1):\n",
    "    file_name = catalog_file_name.format(num)\n",
    "    # print(join(catalog_dir, file_name))\n",
    "    if not os.path.exists(join(catalog_dir, file_name)):\n",
    "        continue\n",
    "    print(file_name)\n",
    "    downloaded_pages = os.listdir(join(webpage_dir, \"{}\".format(num)))\n",
    "    writer = open(join(catalog_dir, failed_file_name).format(num),\n",
    "                  \"w\")\n",
    "\n",
    "    with open(join(catalog_dir, file_name), \"r\") as f:\n",
    "        urls = f.readlines()\n",
    "\n",
    "        for url in urls:\n",
    "            page = url.replace(\"/\", \"_\")[1:].strip() + \".html\"\n",
    "            if page not in downloaded_pages:\n",
    "                writer.write(url)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.kaggle.com\"\n",
    "\n",
    "http_proxy  = \"http://117.90.2.18:9999\"\n",
    "https_proxy = \"https://10.10.1.11:1080\"\n",
    "ftp_proxy   = \"ftp://10.10.1.10:3128\"\n",
    "\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy,\n",
    "            }\n",
    "\n",
    "r = requests.get(url, proxies=proxyDict)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612849"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.getsize(r\"J:\\EECS_6414\\Data\\19-02-24\\kernel_split\\1001\\achal71_gun-violence-analysis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "#/artgor/earthquakes-fe-more-features-and-samples,10737600\n",
    "name = wget.download(url= \"https://www.kaggle.com/kernels/scriptcontent/{}/download\".format(10737600))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'asdf', 'asdfasf']\n"
     ]
    }
   ],
   "source": [
    "print(\"/asdf/asdfasf\".split(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.0.2\n",
      "    Uninstalling pip-19.0.2:\n",
      "      Successfully uninstalled pip-19.0.2\n",
      "Successfully installed pip-19.0.3\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tqdm\n",
    "directory = \"J:\\\\EECS_6414\\\\Data\\\\19-01-17\"\n",
    "total_sum = 0\n",
    "# limit = 200\n",
    "flag = True\n",
    "import pymysql.cursors\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='test_pipeline',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "last_kernelinfo_file = None\n",
    "last_sql = None\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        for i in tqdm.tqdm(os.listdir(directory)):\n",
    "            if \"KernelInfo_\" not in i:\n",
    "                continue\n",
    "            if flag and i != \"KernelInfo_184.json\":\n",
    "                continue\n",
    "            flag = False\n",
    "            print(i)\n",
    "            last_kernelinfo_file = i\n",
    "            with open(os.path.join(directory, i), \"r\", encoding= \"utf-8\") as f:\n",
    "                l = json.load(f)\n",
    "            for kernel in l:\n",
    "#                 total_sum += 1\n",
    "#                 if total_sum >= limit:\n",
    "#                     break\n",
    "\n",
    "                _ = list(kernel.keys())[0]\n",
    "                info = kernel[_]\n",
    "#                 print(_)\n",
    "\n",
    "                a_id = info[\"author\"][\"id\"]\n",
    "                a_name = info[\"author\"][\"userName\"]\n",
    "                last_sql = \"INSERT IGNORE INTO test_pipeline.author (id, name) VALUES ('{}', '{}');\".format(a_id, a_name)\n",
    "                cursor.execute(last_sql)\n",
    "                connection.commit()\n",
    "#                 print(\"author: \", a_id, a_name)\n",
    "\n",
    "                k_id = _\n",
    "                k_name = info[\"scriptUrl\"]\n",
    "                last_sql = \"INSERT IGNORE INTO test_pipeline.kernel (id, name) VALUES ('{}', '{}');\".format(k_id, k_name)\n",
    "                cursor.execute(last_sql)\n",
    "                connection.commit()\n",
    "#                 print(\"kernels: \", k_id, k_name)\n",
    "\n",
    "                datasources = info[\"dataSources\"]\n",
    "                for datasource in datasources:\n",
    "                    ds_id = datasource[\"sourceId\"]\n",
    "                    t_name = datasource[\"dataSourceUrl\"]\n",
    "                    ds_name = t_name if not (t_name == None or t_name == \"\") else \">>|None|<<\"\n",
    "                    ds_type = datasource[\"sourceType\"]\n",
    "                    last_sql = \"INSERT IGNORE INTO test_pipeline.datasource (id, name, type) VALUES (\\\"{}\\\", \\\"{}\\\", \\\"{}\\\");\".format(ds_id, ds_name, ds_type)\n",
    "                    cursor.execute(last_sql)\n",
    "                    connection.commit()\n",
    "#                     print(\"datasourse: \", ds_id, ds_name, ds_type)\n",
    "\n",
    "#                 print(\"\\n\\n\")\n",
    "\n",
    "#             if total_sum >= limit:\n",
    "#                 break\n",
    "finally:\n",
    "    print(\"{} is failed. last sql: {}\".format(last_kernelinfo_file, last_sql))\n",
    "    connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
