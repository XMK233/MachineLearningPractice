{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting and understanding . \n",
    "* V2: The operation process is simplified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Turn the warning off: \n",
    "~~~~\n",
    "options(warn=-1)\n",
    "~~~~\n",
    "* Turn the warning back on: \n",
    "~~~~\n",
    "options(warn=0)\n",
    "~~~~\n",
    "* reference: https://stackoverflow.com/questions/16194212/how-to-suppress-warnings-globally-in-an-r-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step is getting the correlationship between those variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rms)\n",
    "data <- read.csv('J:/EECS6414/process/Pipeline/Data_cleaning/kernel_datassource.csv')# , stringsAsFactors=FALSE\n",
    "\n",
    "col_name <- names(data)\n",
    "env_vars <- col_name\n",
    "vote_num <- data[c(\"km_votes\")]\n",
    "high_voted <- data[c(\"high_voted\")]\n",
    "reject_vars <- c(#\"kernel_datasets\", # because they are all 1, so we can only get NaN. \n",
    "                 \"kernel\", \"author\", \"id\", \"km_tags\", \n",
    "                \"km_votes\", \"high_voted\",  \n",
    "                \"km_forks\", \"km_comments\", \"km_views\"\n",
    "                 #\"competition\", \"organization\" # unnecessary variables. \n",
    "                 )\n",
    "env_vars <- env_vars[!(env_vars %in% reject_vars)]\n",
    "# for (env in env_vars){\n",
    "#    print(paste(env, \"+\"))\n",
    "# }\n",
    "data <- data[env_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars <- c(\n",
    "    \"km_dataSources\", \n",
    "    \"km_Data\", \n",
    "    \"km_versions\", \n",
    "    \"km_dataset_discussion_avg\", \n",
    "      \"km_competition_size_avg\", \n",
    "    \"km_Visualization\", \n",
    "      \"km_tags_te_avg\", \n",
    "      \"km_Other\", \n",
    "      \"km_isNotebook\",\n",
    "      \"km_Hidden\",\n",
    "      \"km_Notebook\",\n",
    "      \"km_null\",\n",
    "      \"km_language\"\n",
    "             )\n",
    "data <- data[env_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reject_vars <- c(\"km_competition_competitor_avg\")\n",
    "env_vars <- env_vars[!(env_vars %in% reject_vars)]\n",
    "data <- data[env_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Step is to prepare the dataframe, add the voted column back to dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary classification: XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.hackerearth.com/blog/machine-learning/beginners-tutorial-on-xgboost-parameter-tuning-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classification = cbind(data, high_voted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>km_dataSources</th><th scope=col>km_Data</th><th scope=col>km_versions</th><th scope=col>km_dataset_discussion_avg</th><th scope=col>km_competition_size_avg</th><th scope=col>km_Visualization</th><th scope=col>km_tags_te_avg</th><th scope=col>km_Other</th><th scope=col>km_isNotebook</th><th scope=col>km_Hidden</th><th scope=col>km_Notebook</th><th scope=col>km_null</th><th scope=col>km_language</th><th scope=col>high_voted</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1         </td><td>1         </td><td>58        </td><td>0         </td><td>2429309862</td><td> 7        </td><td>0.57142598</td><td>0         </td><td>0.09271527</td><td>2         </td><td>1         </td><td>6         </td><td>0.09296084</td><td>True      </td></tr>\n",
       "\t<tr><td>1         </td><td>0         </td><td> 2        </td><td>0         </td><td> 256630325</td><td> 0        </td><td>0.07486423</td><td>0         </td><td>0.09271527</td><td>1         </td><td>1         </td><td>0         </td><td>0.09296084</td><td>True      </td></tr>\n",
       "\t<tr><td>1         </td><td>0         </td><td> 1        </td><td>0         </td><td> 256630325</td><td> 0        </td><td>0.07486423</td><td>0         </td><td>0.09271527</td><td>1         </td><td>1         </td><td>0         </td><td>0.09296084</td><td>True      </td></tr>\n",
       "\t<tr><td>1         </td><td>0         </td><td> 5        </td><td>0         </td><td>     34757</td><td> 2        </td><td>0.50374676</td><td>0         </td><td>0.09271527</td><td>1         </td><td>1         </td><td>0         </td><td>0.09296084</td><td>True      </td></tr>\n",
       "\t<tr><td>1         </td><td>0         </td><td> 3        </td><td>0         </td><td> 256630325</td><td> 0        </td><td>0.07486423</td><td>0         </td><td>0.09271527</td><td>1         </td><td>1         </td><td>0         </td><td>0.09296084</td><td>True      </td></tr>\n",
       "\t<tr><td>1         </td><td>1         </td><td>12        </td><td>0         </td><td> 256630325</td><td>24        </td><td>0.55366252</td><td>0         </td><td>0.09271527</td><td>1         </td><td>1         </td><td>0         </td><td>0.09296084</td><td>True      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       " km\\_dataSources & km\\_Data & km\\_versions & km\\_dataset\\_discussion\\_avg & km\\_competition\\_size\\_avg & km\\_Visualization & km\\_tags\\_te\\_avg & km\\_Other & km\\_isNotebook & km\\_Hidden & km\\_Notebook & km\\_null & km\\_language & high\\_voted\\\\\n",
       "\\hline\n",
       "\t 1          & 1          & 58         & 0          & 2429309862 &  7         & 0.57142598 & 0          & 0.09271527 & 2          & 1          & 6          & 0.09296084 & True      \\\\\n",
       "\t 1          & 0          &  2         & 0          &  256630325 &  0         & 0.07486423 & 0          & 0.09271527 & 1          & 1          & 0          & 0.09296084 & True      \\\\\n",
       "\t 1          & 0          &  1         & 0          &  256630325 &  0         & 0.07486423 & 0          & 0.09271527 & 1          & 1          & 0          & 0.09296084 & True      \\\\\n",
       "\t 1          & 0          &  5         & 0          &      34757 &  2         & 0.50374676 & 0          & 0.09271527 & 1          & 1          & 0          & 0.09296084 & True      \\\\\n",
       "\t 1          & 0          &  3         & 0          &  256630325 &  0         & 0.07486423 & 0          & 0.09271527 & 1          & 1          & 0          & 0.09296084 & True      \\\\\n",
       "\t 1          & 1          & 12         & 0          &  256630325 & 24         & 0.55366252 & 0          & 0.09271527 & 1          & 1          & 0          & 0.09296084 & True      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| km_dataSources | km_Data | km_versions | km_dataset_discussion_avg | km_competition_size_avg | km_Visualization | km_tags_te_avg | km_Other | km_isNotebook | km_Hidden | km_Notebook | km_null | km_language | high_voted |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1          | 1          | 58         | 0          | 2429309862 |  7         | 0.57142598 | 0          | 0.09271527 | 2          | 1          | 6          | 0.09296084 | True       |\n",
       "| 1          | 0          |  2         | 0          |  256630325 |  0         | 0.07486423 | 0          | 0.09271527 | 1          | 1          | 0          | 0.09296084 | True       |\n",
       "| 1          | 0          |  1         | 0          |  256630325 |  0         | 0.07486423 | 0          | 0.09271527 | 1          | 1          | 0          | 0.09296084 | True       |\n",
       "| 1          | 0          |  5         | 0          |      34757 |  2         | 0.50374676 | 0          | 0.09271527 | 1          | 1          | 0          | 0.09296084 | True       |\n",
       "| 1          | 0          |  3         | 0          |  256630325 |  0         | 0.07486423 | 0          | 0.09271527 | 1          | 1          | 0          | 0.09296084 | True       |\n",
       "| 1          | 1          | 12         | 0          |  256630325 | 24         | 0.55366252 | 0          | 0.09271527 | 1          | 1          | 0          | 0.09296084 | True       |\n",
       "\n"
      ],
      "text/plain": [
       "  km_dataSources km_Data km_versions km_dataset_discussion_avg\n",
       "1 1              1       58          0                        \n",
       "2 1              0        2          0                        \n",
       "3 1              0        1          0                        \n",
       "4 1              0        5          0                        \n",
       "5 1              0        3          0                        \n",
       "6 1              1       12          0                        \n",
       "  km_competition_size_avg km_Visualization km_tags_te_avg km_Other\n",
       "1 2429309862               7               0.57142598     0       \n",
       "2  256630325               0               0.07486423     0       \n",
       "3  256630325               0               0.07486423     0       \n",
       "4      34757               2               0.50374676     0       \n",
       "5  256630325               0               0.07486423     0       \n",
       "6  256630325              24               0.55366252     0       \n",
       "  km_isNotebook km_Hidden km_Notebook km_null km_language high_voted\n",
       "1 0.09271527    2         1           6       0.09296084  True      \n",
       "2 0.09271527    1         1           0       0.09296084  True      \n",
       "3 0.09271527    1         1           0       0.09296084  True      \n",
       "4 0.09271527    1         1           0       0.09296084  True      \n",
       "5 0.09271527    1         1           0       0.09296084  True      \n",
       "6 0.09271527    1         1           0       0.09296084  True      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(data_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is used for generating the binary label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then Divide the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 75% of the sample size\n",
    "smp_size <- floor(0.75 * nrow(data_classification))\n",
    "\n",
    "## set the seed to make your partition reproducible\n",
    "set.seed(123)\n",
    "train_ind <- sample(seq_len(nrow(data_classification)), size = smp_size)\n",
    "\n",
    "train <- data_classification[train_ind, ]\n",
    "test <- data_classification[-train_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)\n",
    "setDT(train)\n",
    "setDT(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels <- train$high_voted\n",
    "ts_label <- test$high_voted\n",
    "new_tr <- model.matrix(~.+0,data = train[,-c(\"high_voted\"),with=F])\n",
    "new_ts <- model.matrix(~.+0,data = test[,-c(\"high_voted\"),with=F])\n",
    "labels <- as.numeric(labels)-1\n",
    "ts_label <- as.numeric(ts_label)-1\n",
    "dtrain <- xgb.DMatrix(data = new_tr,label = labels)\n",
    "dtest <- xgb.DMatrix(data = new_ts,label=ts_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params <- list(\n",
    "        booster = \"gbtree\",\n",
    "        objective = \"binary:logistic\",\n",
    "        eta=0.3,\n",
    "        gamma=0,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        subsample=1,\n",
    "        colsample_bytree=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-error:0.085802+0.000441\ttest-error:0.087067+0.001473 \n",
      "Multiple eval metrics are present. Will use test_error for early stopping.\n",
      "Will train until test_error hasn't improved in 20 rounds.\n",
      "\n",
      "[11]\ttrain-error:0.083474+0.000517\ttest-error:0.085462+0.001867 \n",
      "[21]\ttrain-error:0.081841+0.000465\ttest-error:0.084731+0.001548 \n",
      "[31]\ttrain-error:0.080353+0.000442\ttest-error:0.084332+0.001321 \n",
      "[41]\ttrain-error:0.078935+0.000251\ttest-error:0.083980+0.001335 \n",
      "[51]\ttrain-error:0.077740+0.000250\ttest-error:0.083804+0.001390 \n",
      "[61]\ttrain-error:0.076481+0.000386\ttest-error:0.083628+0.001544 \n",
      "[71]\ttrain-error:0.075254+0.000239\ttest-error:0.083675+0.001717 \n",
      "Stopping. Best iteration:\n",
      "[57]\ttrain-error:0.076787+0.000406\ttest-error:0.083493+0.001461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(xgboost)\n",
    "xgbcv <- xgb.cv(params = params\n",
    "                ,data = dtrain\n",
    "                ,nrounds = 1000\n",
    "                ,nfold = 5\n",
    "                ,showsd = T\n",
    "                ,stratified = T\n",
    "                ,print.every.n = 10\n",
    "                ,early.stop.round = 20\n",
    "                ,maximize = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tval-error:0.087627\ttrain-error:0.085909 \n",
      "Multiple eval metrics are present. Will use train_error for early stopping.\n",
      "Will train until train_error hasn't improved in 10 rounds.\n",
      "\n",
      "[11]\tval-error:0.085637\ttrain-error:0.083695 \n",
      "[21]\tval-error:0.084662\ttrain-error:0.082281 \n",
      "[31]\tval-error:0.084296\ttrain-error:0.081164 \n",
      "[41]\tval-error:0.083200\ttrain-error:0.079688 \n",
      "[51]\tval-error:0.082997\ttrain-error:0.078179 \n",
      "[57]\tval-error:0.082997\ttrain-error:0.077664 \n"
     ]
    }
   ],
   "source": [
    "xgb1 <- xgb.train(\n",
    "           params = params\n",
    "          ,data = dtrain\n",
    "          ,nrounds = 57\n",
    "          ,watchlist = list(val=dtest,train=dtrain)\n",
    "          ,print.every.n = 10\n",
    "          ,early.stop.round = 10\n",
    "          ,maximize = F\n",
    "          ,eval_metric = \"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbpred <- predict(xgb1,dtest)\n",
    "xgbpred <- ifelse(xgbpred > 0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbpred[xgbpred == 1] <- true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction     0     1\n",
       "         0 43442  3415\n",
       "         1   672  1714\n",
       "                                          \n",
       "               Accuracy : 0.917           \n",
       "                 95% CI : (0.9145, 0.9194)\n",
       "    No Information Rate : 0.8958          \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.4176          \n",
       " Mcnemar's Test P-Value : < 2.2e-16       \n",
       "                                          \n",
       "            Sensitivity : 0.9848          \n",
       "            Specificity : 0.3342          \n",
       "         Pos Pred Value : 0.9271          \n",
       "         Neg Pred Value : 0.7184          \n",
       "             Prevalence : 0.8958          \n",
       "         Detection Rate : 0.8822          \n",
       "   Detection Prevalence : 0.9515          \n",
       "      Balanced Accuracy : 0.6595          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "confusionMatrix(factor(xgbpred), factor(ts_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAXFxcqKio8PDxN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\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat <- xgb.importance(feature_names = colnames(new_tr),model = xgb1)\n",
    "xgb.plot.importance(importance_matrix = mat[1:20]) #first 20 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasted----XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hackerearth.com/blog/machine-learning/beginners-tutorial-on-xgboost-parameter-tuning-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"xgboost\", repos = \"http://cran.us.r-project.org\", dependencies = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if something is wrong, probably you can use this: \n",
    "as.factor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "bst <- xgboost(data = features, label = labels, max_depth = 2, \n",
    "               eta = 1, nthread = 2, nrounds = 2,objective = \"binary:logistic\")\n",
    "xgb.importance(env_vars, model = bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
