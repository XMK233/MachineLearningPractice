{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019年2月20日 This pipeline is suitable when you don't want to move any files from where they were. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_ajax_json(parameter, page_size):\n",
    "    # parameter is the parameter of json url.\n",
    "    # it looks like \"&competition=9988\"\n",
    "    import requests\n",
    "    num = 0\n",
    "    url = \"https://www.kaggle.com/kernels.json?sortBy=hotness&group=everyone{}&pageSize={}\".format(parameter,page_size)\n",
    "    next_url = url\n",
    "    while True:\n",
    "        r = requests.get(next_url,\n",
    "                         auth=('user', 'pass')).json()\n",
    "        if len(r) == 0:\n",
    "            break\n",
    "        elif len(r) > 0 and len(r) < page_size:\n",
    "            num += len(r)\n",
    "            break\n",
    "        else:\n",
    "            num += len(r)\n",
    "            last_id = r[len(r) - 1][\"id\"]\n",
    "            next_url = url + \"&after={}\".format(last_id)\n",
    "    return num\n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\19-02-03\\\\test_pages\\\\dataset_pages\"\n",
    "competition_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\\\\stats\\\\competition_stats.csv\"\n",
    "dataset_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\\\\stats\\\\dataset_stats.csv\"\n",
    "c = open(competition_file, \"w\", encoding=\"utf-8\")\n",
    "c.write(\"competition,organization,size,discussion,competitors\\n\")#\",kernels\" was the last feature.\n",
    "d = open(dataset_file, \"w\", encoding=\"utf-8\")\n",
    "d.write(\"dataset,author,size,discussions,kernels,downloads,views,vote,version\\n\")\n",
    "# dataset: name\n",
    "# author: name\n",
    "# competition: yes or not\n",
    "# size: how many bytes\n",
    "# discussion: how many discussions if applicable. in \"discussion\"\n",
    "# kernel: how many kernels if applicable\n",
    "# vote: how many votes if applicable\n",
    "# version: how many versions of dataset if applicable\n",
    "\n",
    "counter = 0\n",
    "\n",
    "flag = True\n",
    "for page_name in tqdm(os.listdir(Author_path)):\n",
    "    ################################\n",
    "    ### some kernel pages' name is \"kernels_deleted_xxxxxxx\".\n",
    "    ### I don't know why this kind of pages comes into being.\n",
    "    ### maybe this kernel is just deleted. Simple as it is.\n",
    "    ### Just ignore it first.\n",
    "    ################################\n",
    "    # if page_name != \"del=cd0be45190c25545_glove.42b.300d.html\" and flag:\n",
    "    #     continue\n",
    "    if \"kernels_deleted_\" in page_name:\n",
    "        continue\n",
    "    # #### testing:\n",
    "    # if page_name != \"matschiner_nationmaster.html\" and flag:\n",
    "    #     continue\n",
    "    # flag = False\n",
    "    ##################################\n",
    "    author = page_name.split(\".\")[0]\n",
    "    with open(os.path.join(Author_path, page_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    page_source = etree.HTML(html)\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        continue\n",
    "    ############\n",
    "\n",
    "    print(page_name)\n",
    "    try:\n",
    "        _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: json information cannot be parsed\".format(author))\n",
    "        continue\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: cannot get info\".format(author))\n",
    "        continue\n",
    "    #####################\n",
    "    ## some pages will contain kernel page infomation.\n",
    "    ## this is supposed to be the error of scrapy.\n",
    "    ## for now, lets' just skip this kind of error.\n",
    "    #####################\n",
    "    if \"kernel\" in info:\n",
    "        continue\n",
    "    ##########################\n",
    "    ##Some pages will still have problem, so let's just\n",
    "    ## skip them.\n",
    "    ##########################\n",
    "    try:\n",
    "        if \"competitionId\" in info:\n",
    "            # it is a competition\n",
    "            c.write(\"{},{},{},{},{}\\n\".format(#,{}\n",
    "                info[\"competitionSlug\"], info[\"organizationSlug\"],\n",
    "                info[\"databundle\"][\"totalSize\"], info[\"totalDiscussions\"],\n",
    "                info[\"totalCompetitors\"] #,get_ajax_json(\"&competitionId={}\".format(info[\"competitionId\"]), 200)\n",
    "                )\n",
    "            )\n",
    "            pass\n",
    "        else:\n",
    "            d.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                info[\"slug\"],\n",
    "                info[\"owner\"][\"slug\"],info[\"data\"][\"totalSize\"],\n",
    "                info[\"numberOfTopics\"],info[\"numberOfScripts\"],\n",
    "                info[\"numberOfDownloads\"], info[\"numberOfViews\"],\n",
    "                info[\"voteCount\"],info[\"datasetVersionTotalCount\"]\n",
    "                )\n",
    "            )\n",
    "    except:\n",
    "        print(\"{} info is bad.\".format(author))\n",
    "        continue\n",
    "c.close()\n",
    "d.close()\n",
    "print(\"The dataset is хорошо.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这个脚本里面的东西还算靠谱。可以改进。This script is relatively complete. You can furtherly work on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "from tqdm import tqdm \n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\19-02-03\\\\test_pages\\\\author_pages\"\n",
    "csv_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\\\\stats\\\\author_stats.csv\"\n",
    "c = open(csv_file, \"w\")\n",
    "c.write(\"author,competition,cmp_tier,kernel,scpt_tier,discussion,dsc_tier,follower,following\\n\")\n",
    "counter = 0\n",
    "flag = True\n",
    "for page_name in tqdm(os.listdir(Author_path)):\n",
    "    author = page_name.split(\".\")[0]\n",
    "    # # tester:\n",
    "    # if author != \"breakfastpirate\" and flag:\n",
    "    #     continue\n",
    "    # flag = False\n",
    "    # ##############\n",
    "    with open(os.path.join(Author_path, page_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    page_source = etree.HTML(html)\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        continue\n",
    "    ############\n",
    "    _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: cannot get info.\".format(author))\n",
    "        continue\n",
    "    try:\n",
    "        c.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "            info[\"userName\"],\n",
    "            info[\"competitionsSummary\"][\"totalResults\"],info[\"competitionsSummary\"][\"tier\"],\n",
    "            info[\"scriptsSummary\"][\"totalResults\"],info[\"scriptsSummary\"][\"tier\"],\n",
    "            info[\"discussionsSummary\"][\"totalResults\"],info[\"discussionsSummary\"][\"tier\"],\n",
    "            info[\"followers\"][\"count\"], info[\"following\"][\"count\"]\n",
    "            ).replace(\"novice\", \"1\").replace(\"contributor\", \"2\").replace(\"expert\", \"3\").replace(\"grandmaster\", \"5\").replace(\"master\", \"4\")\n",
    "        )\n",
    "    except:\n",
    "        print(\"{} info is bad.\".format(author))\n",
    "        continue\n",
    "c.close()\n",
    "print(\"The author is хорошо.\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there is a problem. If a kernel uses several datasets, we cannot fit all information into measures. How to manage corresponding data in measures? Here is my solution: how many datasets they used, information of the first one dataset (what dataset it is, the type of this dataset). \n",
    "\n",
    "* 2019年2月10日. First version of kernel parse. You can add more features like code and comment features.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "import tqdm\n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\19-02-03\\\\files\"\n",
    "csv_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\\\\stats\\\\kernel_stats.csv\"\n",
    "c = open(csv_file, \"w\", encoding=\"utf-8\")\n",
    "c.write(\"kernel,author,datasets,source1st,sourceType1st,versions,forks,comments,views,votes\\n\")\n",
    "#kernel: \"kernel\"\"slug\"\n",
    "#author: \"kernel\"\"author\"\"userName\"\n",
    "#competition: if this kernel is for competition.\n",
    "#versions: \"menuLinks\", \"title\" is \"Versions\", \"count\"\n",
    "#forks: \"menuLinks\", \"title\" is \"Forks\", \"count\"\n",
    "#comments: \"menuLinks\", \"title\" is \"Comments\", \"count\"\n",
    "#views: \"kernel\"\"viewCount\"\n",
    "#votes: \"kernel\"\"upvoteCount\"\n",
    "#about source: because a kernel may use several datasets, so here I just take the first dataset into consideration.\n",
    "counter = 0\n",
    "\n",
    "for page_name in tqdm.tqdm(os.listdir(Author_path)):\n",
    "    try:\n",
    "        with open(os.path.join(Author_path, page_name + \"\\\\webpages.html\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "        page_source = etree.HTML(html)\n",
    "        ##################\n",
    "        # sometimes, the html parsing can get none.\n",
    "        # just skip this kind of situation\n",
    "        ##################\n",
    "        if page_source == None:\n",
    "            continue\n",
    "    except:\n",
    "        print(\"no such file: {}\".format(page_name))\n",
    "        continue\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        print(\"bad file: {}\".format(page_name))\n",
    "        continue\n",
    "    ############\n",
    "    try:\n",
    "        _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    except:\n",
    "        print(\"cannot parse json: {}\".format(page_name))\n",
    "        continue\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: cannot get info\".format(page_name))\n",
    "        continue\n",
    "    ## about versions and comments\n",
    "    versions = 0\n",
    "    comments = 0\n",
    "    try:\n",
    "        for menulink in info[\"menuLinks\"]:\n",
    "            if menulink[\"title\"] == \"Versions\":\n",
    "                versions = menulink[\"count\"]\n",
    "            elif menulink[\"title\"] == \"Comments\":\n",
    "                comments = menulink[\"count\"]\n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: bad info at version and comments\".format(page_name))\n",
    "        continue\n",
    "    ####about dataset\n",
    "    try:\n",
    "        source = None\n",
    "        sourceType = None\n",
    "        tmp = info[\"dataSources\"]\n",
    "        datasets = len(tmp)\n",
    "        print(page_name)\n",
    "        if tmp != []:\n",
    "            source = tmp[0][\"slug\"]\n",
    "            sourceType = tmp[0][\"sourceType\"]\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: bad info at data sources\".format(page_name))\n",
    "        continue\n",
    "    # saving to file\n",
    "    try:\n",
    "        c.write(\"{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "            info[\"kernel\"][\"slug\"],\n",
    "            info[\"kernel\"][\"author\"][\"userName\"],\n",
    "            datasets, source, sourceType,\n",
    "            versions, info[\"kernel\"][\"forkCount\"], comments,\n",
    "            info[\"kernel\"][\"viewCount\"], info[\"kernel\"][\"upvoteCount\"]\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        print(\"{} failed. Some bad info\".format(page_name))\n",
    "        continue\n",
    "c.close()\n",
    "print(\"The kernel is хорошо..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2019年2月20日 version 2: combine the dataset and the competition. But the premise is to get the dataset and competition result in advance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "def one_shot(stats, sourceType, list_of_feature, names):\n",
    "    # renew the list of list\n",
    "    # and return invalid_flag\n",
    "    list_of_list = []\n",
    "    for i in range(len(list_of_feature)):\n",
    "        list_of_list.append([])\n",
    "    for name in names:\n",
    "        cmp_info = stats[stats[sourceType] == name]\n",
    "        if len(cmp_info) == 0:\n",
    "            print(\"{} this competition has no information\".format(name))\n",
    "            return False, list_of_list\n",
    "        for l, f in zip(list_of_list, list_of_feature):\n",
    "            l.extend(cmp_info[f].tolist())\n",
    "    return True, list_of_list\n",
    "\n",
    "\n",
    "competition_stats = pd.read_csv(\"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2/stats/competition_stats.csv\",encoding ='latin1')\n",
    "competition_stats.loc[-1] = [\"None\", \"None\", 0, 0, 0]  # adding a row\n",
    "competition_stats.reset_index(drop= True, inplace = True)\n",
    "\n",
    "dataset_stats = pd.read_csv(\"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2/stats/dataset_stats.csv\",encoding ='utf-8')\n",
    "dataset_stats.loc[-1] = [\"None\", \"None\", 0, 0, 0, 0, 0, 0, 0]  # adding a row\n",
    "dataset_stats.reset_index(drop= True, inplace = True)\n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\19-02-03\\\\files\"\n",
    "csv_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\\\\stats\\\\kernel_stats_combined.csv\"\n",
    "k = open(csv_file, \"w\", encoding=\"utf-8\")\n",
    "k.write(\"kernel,author,dataSources,versions,forks,comments,views,votes,\" # 8\n",
    "        \"competitions,\" #1 + 3 * 4 = 13\n",
    "            \"competition_size_ttl,competition_size_avg,competition_size_max,competition_size_min,\" \n",
    "            \"competition_discussion_ttl,competition_discussion_avg,competition_discussion_max,competition_discussion_min,\"\n",
    "            \"competition_competitor_ttl,competition_competitor_avg,competition_competitor_max,competition_competitor_min,\"\n",
    "        \"datasets,\" # 1 + 7 * 4 = 29\n",
    "            \"dataset_size_ttl,dataset_size_avg,dataset_size_max,dataset_size_min,\"\n",
    "            \"dataset_discussion_ttl,dataset_discussion_avg,dataset_discussion_max,dataset_discussion_min,\"\n",
    "            \"dataset_kernel_ttl,dataset_kernel_avg,dataset_kernel_max,dataset_kernel_min,\"\n",
    "            \"dataset_download_ttl,dataset_download_avg,dataset_download_max,dataset_download_min,\"\n",
    "            \"dataset_view_ttl,dataset_view_avg,dataset_view_max,dataset_view_min,\"\n",
    "            \"dataset_vote_ttl,dataset_vote_avg,dataset_vote_max,dataset_vote_min,\"\n",
    "            \"dataset_version_ttl,dataset_version_avg,dataset_version_max,dataset_version_min\"\n",
    "        \"\\n\")\n",
    "counter = 0\n",
    "for page_name in tqdm.tqdm(os.listdir(Author_path)):\n",
    "    counter += 1\n",
    "    # if counter > 20:\n",
    "    #     break\n",
    "    try:\n",
    "        with open(os.path.join(Author_path, page_name + \"\\\\webpages.html\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "        page_source = etree.HTML(html)\n",
    "        ##################\n",
    "        # sometimes, the html parsing can get none.\n",
    "        # just skip this kind of situation\n",
    "        ##################\n",
    "        if page_source == None:\n",
    "            continue\n",
    "    except:\n",
    "        print(\"no such file: {}\".format(page_name))\n",
    "        continue\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        print(\"bad file: {}\".format(page_name))\n",
    "        continue\n",
    "    ############\n",
    "    try:\n",
    "        _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    except:\n",
    "        print(\"cannot parse json: {}\".format(page_name))\n",
    "        continue\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: cannot get info\".format(page_name))\n",
    "        continue\n",
    "    ## about versions and comments\n",
    "    versions = 0\n",
    "    comments = 0\n",
    "    try:\n",
    "        for menulink in info[\"menuLinks\"]:\n",
    "            if menulink[\"title\"] == \"Versions\":\n",
    "                versions = menulink[\"count\"]\n",
    "            elif menulink[\"title\"] == \"Comments\":\n",
    "                comments = menulink[\"count\"]\n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: bad info at version and comments\".format(page_name))\n",
    "        continue\n",
    "    ####about dataset\n",
    "    try:\n",
    "        source = None\n",
    "        sourceType = None\n",
    "        tmp = info[\"dataSources\"]\n",
    "        dataSources = len(tmp)\n",
    "        if tmp != []:\n",
    "            source = tmp[0][\"slug\"].replace(\",\", \" \")\n",
    "            sourceType = tmp[0][\"sourceType\"]\n",
    "        ##############\n",
    "        # divide the datasets: competition and dataset\n",
    "        ##############\n",
    "        cpt = []\n",
    "        dts = []\n",
    "        cpt_num = 0\n",
    "        dts_num = 0\n",
    "        for t in tmp:\n",
    "            if t[\"sourceType\"] == \"competition\":\n",
    "                cpt.append(t[\"slug\"])\n",
    "                cpt_num += 1\n",
    "            elif t[\"sourceType\"] == \"dataset\":\n",
    "                dts.append(t[\"slug\"])\n",
    "                dts_num += 1\n",
    "        if cpt == []:\n",
    "            cpt.append(\"None\")\n",
    "        if dts == []:\n",
    "            dts.append(\"None\")\n",
    "        #divide competition\n",
    "\n",
    "        # invalid_flag = False\n",
    "        # csize = []\n",
    "        # cdiscussion = []\n",
    "        # ccompetitor = []\n",
    "        # for c in cpt:\n",
    "        #     cmp_info = competition_stats[competition_stats[\"competition\"] == c]\n",
    "        #     if len(cmp_info) == 0:\n",
    "        #         print(\"{} this competition has no information\".format(c))\n",
    "        #         invalid_flag = True\n",
    "        #         break\n",
    "        #     csize.append(cmp_info[\"size\"])\n",
    "        #     cdiscussion.append(cmp_info[\"discussion\"])\n",
    "        #     ccompetitor.append(cmp_info[\"competitors\"])\n",
    "\n",
    "        valid_flag, cpt_rst = one_shot(competition_stats,\n",
    "                                       \"competition\",\n",
    "                                      [\"size\", \"discussion\", \"competitors\"],\n",
    "                                      cpt)\n",
    "        if not valid_flag:\n",
    "            continue\n",
    "\n",
    "        valid_flag, dts_rst = one_shot(dataset_stats,\n",
    "                                       \"dataset\",\n",
    "                                      [\"size\", \"discussions\", \"kernels\", \"downloads\", \"views\", \"vote\", \"version\"],\n",
    "                                      dts)\n",
    "        if not valid_flag:\n",
    "            continue\n",
    "\n",
    "        # #divide dataset\n",
    "        # invalid_flag = False\n",
    "        # dsize = []\n",
    "        # ddiscussion = []\n",
    "        # dkernel = []\n",
    "        # ddownload = []\n",
    "        # dview = []\n",
    "        # dvote = []\n",
    "        # dversion = []\n",
    "        # for d in dts:\n",
    "        #     dts_info = dataset_stats[dataset_stats[\"dataset\"] == d]\n",
    "        #     if len(dts_info) == 0:\n",
    "        #         print(\"{} this dataset has no information\".format(d))\n",
    "        #         invalid_flag = True\n",
    "        #         break\n",
    "        #     dsize.append(dts_info[\"size\"])\n",
    "        #     ddiscussion.append(dts_info[\"discussions\"])\n",
    "        #     dkernel.append(dts_info[\"kernels\"])\n",
    "        #     ddownload.append(dts_info[\"downloads\"])\n",
    "        #     dview.append(dts_info[\"views\"])\n",
    "        #     dvote.append(dts_info[\"vote\"])\n",
    "        #     dversion.append(dts_info[\"version\"])\n",
    "        # if invalid_flag:\n",
    "        #     continue\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: bad info at data sources\".format(page_name))\n",
    "        continue\n",
    "    # saving to file\n",
    "    try:\n",
    "        k.write((\",\".join([\"{}\"] * 50) + \"\\n\").format(\n",
    "            info[\"kernel\"][\"slug\"], info[\"kernel\"][\"author\"][\"userName\"], dataSources,\n",
    "            versions, info[\"kernel\"][\"forkCount\"], comments, info[\"kernel\"][\"viewCount\"], info[\"kernel\"][\"upvoteCount\"],\n",
    "            ### 42 features to add\n",
    "            ### sum, avg, max, min\n",
    "            cpt_num,\n",
    "            sum(cpt_rst[0]), mean(cpt_rst[0]), max(cpt_rst[0]), min(cpt_rst[0]),\n",
    "            sum(cpt_rst[1]), mean(cpt_rst[1]), max(cpt_rst[1]), min(cpt_rst[1]),\n",
    "            sum(cpt_rst[2]), mean(cpt_rst[2]), max(cpt_rst[2]), min(cpt_rst[2]),\n",
    "\n",
    "            dts_num,\n",
    "            sum(dts_rst[0]), mean(dts_rst[0]), max(dts_rst[0]), min(dts_rst[0]),\n",
    "            sum(dts_rst[1]), mean(dts_rst[1]), max(dts_rst[1]), min(dts_rst[1]),\n",
    "            sum(dts_rst[2]), mean(dts_rst[2]), max(dts_rst[2]), min(dts_rst[2]),\n",
    "            sum(dts_rst[3]), mean(dts_rst[3]), max(dts_rst[3]), min(dts_rst[3]),\n",
    "            sum(dts_rst[4]), mean(dts_rst[4]), max(dts_rst[4]), min(dts_rst[4]),\n",
    "            sum(dts_rst[5]), mean(dts_rst[5]), max(dts_rst[5]), min(dts_rst[5]),\n",
    "            sum(dts_rst[6]), mean(dts_rst[6]), max(dts_rst[6]), min(dts_rst[6]),\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        print(\"{} failed. Some bad info\".format(page_name))\n",
    "        continue\n",
    "k.close()\n",
    "print(\"The kernel is хорошо..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\")\n",
    "author = pd.read_csv(\"stats/author_stats.csv\", encoding=\"utf-8\")\n",
    "author.rename(index=str,\n",
    "              columns={\"competition\": \"author_competitions\",\n",
    "                                  \"kernel\": \"author_kernels\",\n",
    "                                  \"discussion\": \"author_discussions\",\n",
    "                                  \"follower\": \"author_followers\",\n",
    "                                  \"following\": \"author_following\"},\n",
    "              inplace=True)\n",
    "\n",
    "competition = pd.read_csv(\"stats/competition_stats.csv\",encoding ='latin1') ## very strange\n",
    "competition.rename(index=str,\n",
    "             columns={\"discussion\": \"competition_discussion\"#,\n",
    "                      }, #\"kernels\": \"competition_kernels\"\n",
    "             inplace= True)\n",
    "# competition.loc[-1] = [\"None\", \"None\", 0, 0, 0]  # adding a row\n",
    "# competition.reset_index(drop= True, inplace = True)\n",
    "\n",
    "dataset = pd.read_csv(\"stats/dataset_stats.csv\",encoding ='utf-8')\n",
    "dataset.rename(index=str,\n",
    "              columns={\"author\": \"dataset_author\",\n",
    "                                  \"discussions\": \"dataset_discussions\",\n",
    "                                  \"kernels\": \"dataset_kernels\",\n",
    "                                \"downloads\": \"dataset_downloads\",\n",
    "                               \"views\": \"dataset_views\",\n",
    "                               \"vote\": \"dataset_votes\",\n",
    "                                  \"version\": \"dataset_versions\"},\n",
    "              inplace=True)\n",
    "# dataset.loc[-1] = [\"None\", \"None\", 0, 0, 0, 0, 0, 0, 0]  # adding a row\n",
    "# dataset.reset_index(drop= True, inplace = True)\n",
    "\n",
    "kernel = pd.read_csv(\"stats/kernel_stats.csv\", encoding=\"utf-8\")\n",
    "kernel.rename(index=str,\n",
    "             columns={\"datasets\": \"kernel_datasets\",\n",
    "                      \"versions\": \"kernel_versions\"},\n",
    "             inplace= True)\n",
    "##########\n",
    "# I don't know why, there seems to have empty field in \n",
    "# the final csv files. \n",
    "##########\n",
    "kernel.drop([\"Unnamed: 10\", \"Unnamed: 11\"], axis=1, inplace= True)\n",
    "\n",
    "# kernel_competition_author.csv generating:\n",
    "kernel_competition = kernel[kernel[\"sourceType1st\"]==\"competition\"]\n",
    "kernel_competition = kernel_competition.drop(['sourceType1st'], axis=1)\n",
    "kernel_competition.rename(index=str, columns={\"source1st\": \"competition\"}, inplace=True)\n",
    "\n",
    "a = pd.merge(kernel_competition, competition, on='competition')\n",
    "b = pd.merge(a, author, on='author')\n",
    "b.to_csv(\"kernel_competition_author.csv\", index=None)\n",
    "\n",
    "# kernel_dataset_author.csv generating:\n",
    "kernel_dataset = kernel[kernel[\"sourceType1st\"]==\"dataset\"]\n",
    "kernel_dataset = kernel_dataset.drop(['sourceType1st'], axis=1)\n",
    "kernel_dataset.rename(index=str, columns={\"source1st\": \"dataset\"}, inplace=True)\n",
    "\n",
    "a = pd.merge(kernel_dataset, dataset, on='dataset')\n",
    "b = pd.merge(a, author, on='author')\n",
    "b.to_csv(\"kernel_dataset_author.csv\", index=None)\n",
    "\n",
    "# kernel_nodataset_author.csv generating:\n",
    "kernel_nodataset = kernel[kernel[\"sourceType1st\"]==\"None\"]\n",
    "kernel_nodataset = kernel_nodataset.drop(['sourceType1st', \"source1st\"], axis=1)\n",
    "\n",
    "a = pd.merge(kernel_nodataset, author, on='author')\n",
    "a.to_csv(\"kernel_nodataset_author.csv\", index=None)\n",
    "\n",
    "print(\"хорошо!!!! Let's do the training. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* version 2. Use the kernel_stats_combined.csv file and author_stats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хорошо!!!! Let's do the training. \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2\")\n",
    "author = pd.read_csv(\"stats/author_stats.csv\", encoding=\"utf-8\")\n",
    "author.rename(index=str,\n",
    "              columns={\"competition\": \"author_competitions\",\n",
    "                                  \"kernel\": \"author_kernels\",\n",
    "                                  \"discussion\": \"author_discussions\",\n",
    "                                  \"follower\": \"author_followers\",\n",
    "                                  \"following\": \"author_following\"},\n",
    "              inplace=True)\n",
    "\n",
    "kernel = pd.read_csv(\"stats/kernel_stats_combined.csv\", encoding=\"utf-8\")\n",
    "# kernel.rename(index=str,\n",
    "#              columns={\"datasets\": \"kernel_datasets\",\n",
    "#                       \"versions\": \"kernel_versions\"},\n",
    "#              inplace= True)\n",
    "b = pd.merge(kernel, author, on='author')\n",
    "b.to_csv(\"kernel_cpt_dts_author.csv\", index=None)\n",
    "\n",
    "print(\"хорошо!!!! Let's do the training. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "competition_stats = pd.read_csv(\"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2/stats/competition_stats.csv\",encoding ='latin1')\n",
    "dataset_stats = pd.read_csv(\"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET2/stats/dataset_stats.csv\",encoding ='utf-8')\n",
    "\n",
    "if len(competition_stats[competition_stats[\"competition\"] == \"2018-hse-ml-competion-2\"]) == 0:\n",
    "    print(\"cannot find\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = competition_stats[competition_stats[\"competition\"] == \"2018-hse-ml-competion-0\"][\"size\"].tolist()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competition</th>\n",
       "      <th>organization</th>\n",
       "      <th>size</th>\n",
       "      <th>discussion</th>\n",
       "      <th>competitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20-newsgroups-ciphertext-challenge</td>\n",
       "      <td>kaggle</td>\n",
       "      <td>24871600</td>\n",
       "      <td>20</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018 Travelers Statistical Modeling Competition</td>\n",
       "      <td>None</td>\n",
       "      <td>1304488</td>\n",
       "      <td>7</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-hse-ml-competion-02</td>\n",
       "      <td>None</td>\n",
       "      <td>695347899</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-hse-ml-competion-03</td>\n",
       "      <td>None</td>\n",
       "      <td>1105448927</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-1st-ml-month-with-kakr</td>\n",
       "      <td>None</td>\n",
       "      <td>34131</td>\n",
       "      <td>10</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       competition organization        size  \\\n",
       "0               20-newsgroups-ciphertext-challenge       kaggle    24871600   \n",
       "1  2018 Travelers Statistical Modeling Competition         None     1304488   \n",
       "2                         2018-hse-ml-competion-02         None   695347899   \n",
       "3                         2018-hse-ml-competion-03         None  1105448927   \n",
       "4                      2019-1st-ml-month-with-kakr         None       34131   \n",
       "\n",
       "   discussion  competitors  \n",
       "0          20          149  \n",
       "1           7          115  \n",
       "2           0          126  \n",
       "3           0          171  \n",
       "4          10          374  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join([\"{}\"] * 50) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2], [], []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = []\n",
    "for i in range(3):\n",
    "    k.append([])\n",
    "print(k)\n",
    "k[0].append(1)\n",
    "k[0].append(2)\n",
    "k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
