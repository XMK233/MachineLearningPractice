{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude Kerneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys, os, json\n",
    "DATA_STORE = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\"\n",
    "\n",
    "kernel_ids = []\n",
    "counter = 0\n",
    "limit = 3\n",
    "for fn in os.listdir(DATA_STORE):\n",
    "    if \"KernelInfo\" not in fn:\n",
    "        continue\n",
    "    print(fn)\n",
    "    # counter += 1\n",
    "    if counter >= limit:\n",
    "        break\n",
    "    #####\n",
    "    with open(os.path.join(DATA_STORE, fn), \"r\") as f:\n",
    "        kernels = json.load(f)\n",
    "    for kernel in kernels:\n",
    "        # key = list(kernel.keys())[0]\n",
    "        value = kernel[\"info\"]\n",
    "        if value[\"author\"][\"displayName\"] == \"Kaggle Kerneler\":\n",
    "            continue\n",
    "        kernel_ids.append(kernel)\n",
    "        # counter+=1\n",
    "\n",
    "with open(os.path.join(DATA_STORE, \"combined_v1.json\"), \"w\") as f:\n",
    "    kernels = json.dump(kernel_ids, f)\n",
    "print(len(kernel_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get files of kernels for this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json, os, shutil\n",
    "json_path = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\combined_v1.json\"\n",
    "file_from = \"J:\\\\EECS_6414\\\\Data\\\\19-02-03\\\\files\"\n",
    "file_to = \"J:\\EECS_6414\\Data\\TEST_DATASET\\\\files\"\n",
    "if not os.path.exists(file_to):\n",
    "    os.makedirs(file_to)\n",
    "with open(json_path, \"r\") as j:\n",
    "    kernels = json.load(j)\n",
    "for kernel in kernels:\n",
    "   title = kernel[\"title\"]\n",
    "   shutil.copytree(os.path.join(file_from, title),\n",
    "                   os.path.join(file_to, title))\n",
    "print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Authors and Datasets from Kaggle using Spider in \"crawler\"\n",
    "* I don't know why, only 1 item in all of the 2 will be download. Whatever, this is not the top concern now. \n",
    "* 2019年2月9日 in terms of the result I got now, I can say that the author gathering is OK now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这个脚本里面的东西还算靠谱。可以改进。This script is relatively complete. You can furtherly work on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\test_pages\\\\author_pages\"\n",
    "csv_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\author_stats.csv\"\n",
    "c = open(csv_file, \"w\")\n",
    "c.write(\"author,competition,cmp_tier,kernel,scpt_tier,discussion,dsc_tier,follower,following\\n\")\n",
    "counter = 0\n",
    "\n",
    "for page_name in os.listdir(Author_path):\n",
    "    author = page_name.split(\".\")[0]\n",
    "    with open(os.path.join(Author_path, page_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    page_source = etree.HTML(html)\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        continue\n",
    "    ############\n",
    "    _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: such information cannot be parsed\".format(author))\n",
    "\n",
    "    c.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "        info[\"userName\"],\n",
    "        info[\"competitionsSummary\"][\"totalResults\"],info[\"competitionsSummary\"][\"tier\"],\n",
    "        info[\"scriptsSummary\"][\"totalResults\"],info[\"scriptsSummary\"][\"tier\"],\n",
    "        info[\"discussionsSummary\"][\"totalResults\"],info[\"discussionsSummary\"][\"tier\"],\n",
    "        info[\"followers\"][\"count\"], info[\"following\"][\"count\"]\n",
    "        ).replace(\"novice\", \"1\").replace(\"contributor\", \"2\").replace(\"expert\", \"3\").replace(\"grandmaster\", \"5\").replace(\"master\", \"4\")                                                    \n",
    "    )\n",
    "c.close()\n",
    "print(\"хорошо, total number is: {}\".format(counter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2019年2月10日. This is OK now. Can be further modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "\n",
    "def get_ajax_json(parameter, page_size):\n",
    "    # parameter is the parameter of json url.\n",
    "    # it looks like \"&competition=9988\"\n",
    "    import requests\n",
    "    num = 0\n",
    "    url = \"https://www.kaggle.com/kernels.json?sortBy=hotness&group=everyone{}&pageSize={}\".format(parameter,page_size)\n",
    "    next_url = url\n",
    "    while True:\n",
    "        r = requests.get(next_url,\n",
    "                         auth=('user', 'pass')).json()\n",
    "        if len(r) == 0:\n",
    "            break\n",
    "        elif len(r) > 0 and len(r) < page_size:\n",
    "            num += len(r)\n",
    "            break\n",
    "        else:\n",
    "            num += len(r)\n",
    "            last_id = r[len(r) - 1][\"id\"]\n",
    "            next_url = url + \"&after={}\".format(last_id)\n",
    "    return num\n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\test_pages\\\\dataset_pages\"\n",
    "competition_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\competition_stats.csv\"\n",
    "dataset_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\dataset_stats.csv\"\n",
    "c = open(competition_file, \"w\")\n",
    "c.write(\"competition,organization,size,discussion,competitors,kernels\\n\")\n",
    "d = open(dataset_file, \"w\")\n",
    "d.write(\"dataset,author,size,discussions,kernels,downloads,views,vote,version\\n\")\n",
    "# dataset: name\n",
    "# author: name\n",
    "# competition: yes or not\n",
    "# size: how many bytes\n",
    "# discussion: how many discussions if applicable. in \"discussion\"\n",
    "# kernel: how many kernels if applicable\n",
    "# vote: how many votes if applicable\n",
    "# version: how many versions of dataset if applicable\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for page_name in os.listdir(Author_path):\n",
    "    ################################\n",
    "    ### some kernel pages' name is \"kernels_deleted_xxxxxxx\".\n",
    "    ### I don't know why this kind of pages comes into being.\n",
    "    ### maybe this kernel is just deleted. Simple as it is.\n",
    "    ### Just ignore it first.\n",
    "    ################################\n",
    "    if \"kernels_deleted_\" in page_name:\n",
    "        continue\n",
    "\n",
    "    author = page_name.split(\".\")[0]\n",
    "    with open(os.path.join(Author_path, page_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    page_source = etree.HTML(html)\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        continue\n",
    "    ############\n",
    "    _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: such information cannot be parsed\".format(author))\n",
    "\n",
    "    #####################\n",
    "    ## some pages will contain kernel page infomation.\n",
    "    ## this is supposed to be the error of scrapy.\n",
    "    ## for now, lets' just skip this kind of error.\n",
    "    #####################\n",
    "    if \"kernel\" in info:\n",
    "        continue\n",
    "\n",
    "    print(page_name)\n",
    "    if \"competitionId\" in info:\n",
    "        # it is a competition\n",
    "        c.write(\"{},{},{},{},{},{}\\n\".format(\n",
    "            info[\"competitionSlug\"], info[\"organizationSlug\"],\n",
    "            info[\"databundle\"][\"totalSize\"], info[\"totalDiscussions\"],\n",
    "            info[\"totalCompetitors\"],get_ajax_json(\"&competitionId={}\".format(info[\"competitionId\"]), 200)\n",
    "            )\n",
    "        )\n",
    "        pass\n",
    "    else:\n",
    "        d.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "            info[\"slug\"],\n",
    "            info[\"owner\"][\"slug\"],info[\"data\"][\"totalSize\"],\n",
    "            info[\"numberOfTopics\"],info[\"numberOfScripts\"],\n",
    "            info[\"numberOfDownloads\"], info[\"numberOfViews\"],\n",
    "            info[\"voteCount\"],info[\"datasetVersionTotalCount\"]\n",
    "            )\n",
    "        )\n",
    "c.close()\n",
    "d.close()\n",
    "print(\"хорошо\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Kernel\n",
    "* there is a problem. If a kernel uses several datasets, we cannot fit all information into measures. How to manage corresponding data in measures? Here is my solution: how many datasets they used, information of the first one dataset (what dataset it is, the type of this dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2019年2月10日. First version of kernel parse. You can add more features like code and comment features.              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil, re\n",
    "from lxml import etree\n",
    "\n",
    "Author_path = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\files\"\n",
    "csv_file = \"J:\\\\EECS_6414\\\\Data\\\\TEST_DATASET\\\\kernel_stats.csv\"\n",
    "c = open(csv_file, \"w\")\n",
    "c.write(\"kernel,author,datasets,source1st,sourceType1st,versions,forks,comments,views,votes\\n\")\n",
    "#kernel: \"kernel\"\"slug\"\n",
    "#author: \"kernel\"\"author\"\"userName\"\n",
    "#competition: if this kernel is for competition.\n",
    "#versions: \"menuLinks\", \"title\" is \"Versions\", \"count\"\n",
    "#forks: \"menuLinks\", \"title\" is \"Forks\", \"count\"\n",
    "#comments: \"menuLinks\", \"title\" is \"Comments\", \"count\"\n",
    "#views: \"kernel\"\"viewCount\"\n",
    "#votes: \"kernel\"\"upvoteCount\"\n",
    "#about source: because a kernel may use several datasets, so here I just take the first dataset into consideration.\n",
    "counter = 0\n",
    "\n",
    "for page_name in os.listdir(Author_path):\n",
    "    try:\n",
    "        with open(os.path.join(Author_path, page_name + \"\\\\webpages.html\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "        page_source = etree.HTML(html)\n",
    "    except:\n",
    "        print(\"no such file: {}\".format(page_name))\n",
    "    ###########\n",
    "    # this part is for dealing with the problem\n",
    "    # of: too many request.\n",
    "    # we have to abondon those \"too many requests\" pages temprarily.\n",
    "    ###########\n",
    "    if page_source.xpath(\"/html/body/pre/text()\") != []:\n",
    "        continue\n",
    "    ############\n",
    "    _ = page_source.xpath(\"//div[@class='site-layout__main-content']/script[1]/text()\")[0]\n",
    "    p = re.compile(\"Kaggle\\.State\\.push\\(\\{[\\s\\S]*\\}\\)\\;\")\n",
    "    try:\n",
    "        s = p.findall(_)[0].replace(\"Kaggle.State.push(\", \"\")[0:-2]\n",
    "        info = json.loads(s)\n",
    "    except:\n",
    "        ##############################\n",
    "        # this place occurs because\n",
    "        # we will fail to crawl such information\n",
    "        ##############################\n",
    "        print(\"{} failed: such information cannot be parsed\".format(page_name))\n",
    "    ## about versions and comments\n",
    "    versions = 0\n",
    "    comments = 0\n",
    "    for menulink in info[\"menuLinks\"]:\n",
    "        if menulink[\"title\"] == \"Versions\":\n",
    "            versions = menulink[\"count\"]\n",
    "        elif menulink[\"title\"] == \"Comments\":\n",
    "            comments = menulink[\"count\"]\n",
    "        else:\n",
    "            pass\n",
    "    ####about dataset\n",
    "    source = None\n",
    "    sourceType = None\n",
    "    tmp = info[\"dataSources\"]\n",
    "    datasets = len(tmp)\n",
    "    print(page_name)\n",
    "    if tmp != []:\n",
    "        source = tmp[0][\"slug\"]\n",
    "        sourceType = tmp[0][\"sourceType\"]\n",
    "    # saving to file\n",
    "    c.write(\"{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "        info[\"kernel\"][\"slug\"],\n",
    "        info[\"kernel\"][\"author\"][\"userName\"],\n",
    "        datasets, source, sourceType,\n",
    "        versions, info[\"kernel\"][\"forkCount\"], comments,\n",
    "        info[\"kernel\"][\"viewCount\"], info[\"kernel\"][\"upvoteCount\"]\n",
    "        )\n",
    "    )\n",
    "c.close()\n",
    "print(\"хорошо, total number is: {}\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "author = pd.read_csv(\"stats/author_stats.csv\", encoding=\"utf-8\")\n",
    "author.rename(index=str, \n",
    "              columns={\"competition\": \"author_competitions\", \n",
    "                                  \"kernel\": \"author_kernels\", \n",
    "                                  \"discussion\": \"author_discussions\", \n",
    "                                  \"follower\": \"author_followers\",\n",
    "                                  \"following\": \"author_following\"}, \n",
    "              inplace=True)\n",
    "\n",
    "competition = pd.read_csv(\"stats/competition_stats.csv\",encoding ='latin1') ## very strange\n",
    "competition.rename(index=str, \n",
    "             columns={\"discussion\": \"competition_discussion\", \n",
    "                      \"kernels\": \"competition_kernels\"}, \n",
    "             inplace= True)\n",
    "\n",
    "dataset = pd.read_csv(\"stats/dataset_stats.csv\",encoding ='utf-8')\n",
    "dataset.rename(index=str, \n",
    "              columns={\"author\": \"dataset_author\", \n",
    "                                  \"discussions\": \"dataset_discussions\", \n",
    "                                  \"kernels\": \"dataset_kernels\", \n",
    "                                \"downloads\": \"dataset_downloads\", \n",
    "                               \"views\": \"dataset_views\", \n",
    "                               \"vote\": \"dataset_votes\", \n",
    "                                  \"version\": \"dataset_versions\"}, \n",
    "              inplace=True)\n",
    "\n",
    "kernel = pd.read_csv(\"stats/kernel_stats.csv\", encoding=\"utf-8\")\n",
    "kernel.rename(index=str, \n",
    "             columns={\"datasets\": \"kernel_datasets\", \n",
    "                      \"versions\": \"kernel_versions\"}, \n",
    "             inplace= True)\n",
    "\n",
    "# kernel_competition_author.csv generating: \n",
    "kernel_competition = kernel[kernel[\"sourceType1st\"]==\"competition\"]\n",
    "kernel_competition = kernel_competition.drop(['sourceType1st'], axis=1)\n",
    "kernel_competition.rename(index=str, columns={\"source1st\": \"competition\"}, inplace=True)\n",
    "\n",
    "a = pd.merge(kernel_competition, competition, on='competition')\n",
    "b = pd.merge(a, author, on='author')\n",
    "b.to_csv(\"kernel_competition_author.csv\", index=None)\n",
    "\n",
    "# kernel_dataset_author.csv generating: \n",
    "kernel_dataset = kernel[kernel[\"sourceType1st\"]==\"dataset\"]\n",
    "kernel_dataset = kernel_dataset.drop(['sourceType1st'], axis=1)\n",
    "kernel_dataset.rename(index=str, columns={\"source1st\": \"dataset\"}, inplace=True)\n",
    "\n",
    "a = pd.merge(kernel_dataset, dataset, on='dataset')\n",
    "b = pd.merge(a, author, on='author')\n",
    "b.to_csv(\"kernel_dataset_author.csv\", index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
