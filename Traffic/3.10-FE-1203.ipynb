{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('originalDataset/train.csv')\n",
    "data_test_a = pd.read_csv('originalDataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in data_train.columns if f not in ['CaseId', 'Evaluation', \"policyCode\", \"issueDate\", \"earliesCreditLine\"] and '_outliers' not in f]\n",
    "y_train = data_train['Evaluation']\n",
    "x_train = data_train[features]\n",
    "x_test = data_test_a[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aucprFunction(pred, train_data):\n",
    "    '''\n",
    "    如果是给lgbm用的，参考https://github.com/microsoft/LightGBM/blob/c02917e493c36f3b1e349338f1087fed33126576/examples/python-guide/advanced_example.py#L154\n",
    "    第一个返回值，是这个函数的可以说是名字或者是标记吧；score就是得到的分数；最后一个就是问，score是越高越好吗。\n",
    "    '''\n",
    "    labels = train_data.get_label()\n",
    "    score = average_precision_score(labels, pred)\n",
    "    return \"AUC_PR\", score, True\n",
    "\n",
    "class AUCPR_forCAT(object):\n",
    "    '''\n",
    "    https://catboost.ai/docs/concepts/python-usages-examples.html#custom-loss-function-eval-metric\n",
    "    '''\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        # approxes is a list of indexed containers\n",
    "        # (containers with only __len__ and __getitem__ defined),\n",
    "        # one container per approx dimension.\n",
    "        # Each container contains floats.\n",
    "        # weight is a one dimensional indexed container.\n",
    "        # target is a one dimensional indexed container.\n",
    "        \n",
    "        # weight parameter can be None.\n",
    "        # Returns pair (error, weights sum)\n",
    "\n",
    "        ## weight没什么迪奥用。\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "        approx = approxes[0]\n",
    "        score = average_precision_score(list(target), approx)\n",
    "        return score, -1\n",
    "    \n",
    "    def get_final_error(self, error, weight):\n",
    "        # Returns final value of metric based on error and weight\n",
    "        return error\n",
    "\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name, folds = 5):\n",
    "    # folds = 5\n",
    "    seed = 2020\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    train = np.zeros(train_x.shape[0])\n",
    "    test = np.zeros(test_x.shape[0])\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y.iloc[train_index], train_x.iloc[valid_index], train_y.iloc[valid_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt', 'objective': 'binary',\n",
    "                # 'metric': 'auc',\n",
    "                'min_child_weight': 5, 'num_leaves': 2 ** 5, 'lambda_l2': 10, 'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8, 'bagging_freq': 4, 'learning_rate': 0.1, 'seed': 2020,\n",
    "                'nthread': 28, 'n_jobs':24, 'silent': True, 'verbose': -1,    \n",
    "#                 'device_type': 'gpu',\n",
    "#                 'max_bin': 63,\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200, feval=aucprFunction)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "            \n",
    "            # print(list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "                \n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            \n",
    "            params = {'booster': 'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'gamma': 1,\n",
    "                      'min_child_weight': 1.5, 'max_depth': 5, 'lambda': 10, 'subsample': 0.7, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.04, 'tree_method': 'exact', # \"gpu_hist\", #\n",
    "                      'seed': 2020, 'nthread': 36, \"silent\": True,\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "            # https://stackoverflow.com/questions/55579610/xgboost-attributeerror-dataframe-object-has-no-attribute-feature-names\n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(xgb.DMatrix(test_x) , ntree_limit=model.best_ntree_limit)\n",
    "                 \n",
    "        if clf_name == \"cat\":\n",
    "            ## metrics函数：https://catboost.ai/docs/references/eval-metric__supported-metrics.html \n",
    "            params = {'learning_rate': 0.05, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "                      'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False, \"task_type\": \"GPU\"} #, \"eval_metric\": \"Kappa\"\n",
    "            \n",
    "            model = clf(iterations=20000, **params)#, eval_metric = AUCPR_forCAT()\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      cat_features=[], use_best_model=True, verbose=500)\n",
    "            \n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "            \n",
    "        ## 下面这个test，可以考虑像如下这样进行，或者是求平均亦可。\n",
    "        # test += test_pred / kf.n_splits ## Minke's invention, no basis or reference.\n",
    "        test += test_pred\n",
    "        ##\n",
    "        train[valid_index] = val_pred\n",
    "        cv_scores.append(average_precision_score(val_y, val_pred))\n",
    "        \n",
    "        print(sum(cv_scores)/len(cv_scores), cv_scores)\n",
    "        \n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    print(\"what is kf.n_splits?\", kf.n_splits)\n",
    "    return train, test / kf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(x_train, y_train, x_test):\n",
    "    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\")\n",
    "    return lgb_train, lgb_test\n",
    "\n",
    "def xgb_model(x_train, y_train, x_test):\n",
    "    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\")\n",
    "    return xgb_train, xgb_test\n",
    "\n",
    "def cat_model(x_train, y_train, x_test):\n",
    "    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, \"cat\")\n",
    "    return cat_train, cat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.159898\ttraining's AUC_PR: 0.86351\tvalid_1's binary_logloss: 0.16623\tvalid_1's AUC_PR: 0.853332\n",
      "[400]\ttraining's binary_logloss: 0.15819\ttraining's AUC_PR: 0.865628\tvalid_1's binary_logloss: 0.165701\tvalid_1's AUC_PR: 0.853618\n",
      "Early stopping, best iteration is:\n",
      "[371]\ttraining's binary_logloss: 0.158361\ttraining's AUC_PR: 0.865472\tvalid_1's binary_logloss: 0.165688\tvalid_1's AUC_PR: 0.853824\n",
      "0.8538241432478658 [0.8538241432478658]\n",
      "************************************ 2 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.160493\ttraining's AUC_PR: 0.862722\tvalid_1's binary_logloss: 0.163383\tvalid_1's AUC_PR: 0.856771\n",
      "[400]\ttraining's binary_logloss: 0.158726\ttraining's AUC_PR: 0.864915\tvalid_1's binary_logloss: 0.162932\tvalid_1's AUC_PR: 0.857028\n",
      "Early stopping, best iteration is:\n",
      "[335]\ttraining's binary_logloss: 0.159134\ttraining's AUC_PR: 0.864428\tvalid_1's binary_logloss: 0.162986\tvalid_1's AUC_PR: 0.857149\n",
      "0.8554866291720047 [0.8538241432478658, 0.8571491150961436]\n",
      "************************************ 3 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.160764\ttraining's AUC_PR: 0.861882\tvalid_1's binary_logloss: 0.162108\tvalid_1's AUC_PR: 0.860712\n",
      "[400]\ttraining's binary_logloss: 0.15893\ttraining's AUC_PR: 0.864249\tvalid_1's binary_logloss: 0.161814\tvalid_1's AUC_PR: 0.860865\n",
      "Early stopping, best iteration is:\n",
      "[266]\ttraining's binary_logloss: 0.159918\ttraining's AUC_PR: 0.862903\tvalid_1's binary_logloss: 0.161838\tvalid_1's AUC_PR: 0.861171\n",
      "0.8573812631297045 [0.8538241432478658, 0.8571491150961436, 0.8611705310451043]\n",
      "************************************ 4 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.160645\ttraining's AUC_PR: 0.862504\tvalid_1's binary_logloss: 0.162679\tvalid_1's AUC_PR: 0.857761\n",
      "[400]\ttraining's binary_logloss: 0.15885\ttraining's AUC_PR: 0.864671\tvalid_1's binary_logloss: 0.162159\tvalid_1's AUC_PR: 0.858347\n",
      "[600]\ttraining's binary_logloss: 0.157955\ttraining's AUC_PR: 0.865891\tvalid_1's binary_logloss: 0.162132\tvalid_1's AUC_PR: 0.858194\n",
      "Early stopping, best iteration is:\n",
      "[455]\ttraining's binary_logloss: 0.158569\ttraining's AUC_PR: 0.865023\tvalid_1's binary_logloss: 0.16205\tvalid_1's AUC_PR: 0.85856\n",
      "0.8576759581472565 [0.8538241432478658, 0.8571491150961436, 0.8611705310451043, 0.8585600431999124]\n",
      "************************************ 5 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.160971\ttraining's AUC_PR: 0.861523\tvalid_1's binary_logloss: 0.161278\tvalid_1's AUC_PR: 0.861736\n",
      "[400]\ttraining's binary_logloss: 0.159132\ttraining's AUC_PR: 0.863786\tvalid_1's binary_logloss: 0.160978\tvalid_1's AUC_PR: 0.861165\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's binary_logloss: 0.160434\ttraining's AUC_PR: 0.862212\tvalid_1's binary_logloss: 0.161049\tvalid_1's AUC_PR: 0.862059\n",
      "0.8585525102124472 [0.8538241432478658, 0.8571491150961436, 0.8611705310451043, 0.8585600431999124, 0.8620587184732099]\n",
      "lgb_scotrainre_list: [0.8538241432478658, 0.8571491150961436, 0.8611705310451043, 0.8585600431999124, 0.8620587184732099]\n",
      "lgb_score_mean: 0.8585525102124472\n",
      "lgb_score_std: 0.0029487135567286644\n",
      "what is kf.n_splits? 5\n"
     ]
    }
   ],
   "source": [
    "lgb_train, lgb_test = lgb_model(x_train, y_train, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
